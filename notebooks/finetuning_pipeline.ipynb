{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-15T22:07:27.792007Z","iopub.status.busy":"2024-02-15T22:07:27.791306Z","iopub.status.idle":"2024-02-15T22:07:28.164415Z","shell.execute_reply":"2024-02-15T22:07:28.163406Z","shell.execute_reply.started":"2024-02-15T22:07:27.791969Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('data/transcript_files'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T22:07:28.166505Z","iopub.status.busy":"2024-02-15T22:07:28.166115Z","iopub.status.idle":"2024-02-15T22:07:35.939756Z","shell.execute_reply":"2024-02-15T22:07:35.938887Z","shell.execute_reply.started":"2024-02-15T22:07:28.166479Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/aryamansriram/micromamba/envs/llama_fun/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","config.json: 100%|██████████| 762/762 [00:00<00:00, 299kB/s]\n","vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 7.95MB/s]\n","merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 50.9MB/s]\n","tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 11.6MB/s]\n"]}],"source":["from transformers import AutoModelForCausalLM,AutoTokenizer,TrainingArguments,Trainer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T22:07:35.942160Z","iopub.status.busy":"2024-02-15T22:07:35.941240Z","iopub.status.idle":"2024-02-15T22:07:36.319830Z","shell.execute_reply":"2024-02-15T22:07:36.318923Z","shell.execute_reply.started":"2024-02-15T22:07:35.942130Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating train split: 56 examples [00:00, 2869.16 examples/s]\n"]}],"source":["from datasets import load_dataset\n","data = load_dataset(\"text\",data_dir=\"../src/data/transcript_files/\")\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T22:07:36.322143Z","iopub.status.busy":"2024-02-15T22:07:36.321862Z","iopub.status.idle":"2024-02-15T22:07:37.653393Z","shell.execute_reply":"2024-02-15T22:07:37.652577Z","shell.execute_reply.started":"2024-02-15T22:07:36.322118Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Map:   0%|          | 0/56 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (15558 > 1024). Running this sequence through the model will result in indexing errors\n","Map: 100%|██████████| 56/56 [00:01<00:00, 38.82 examples/s]\n"]}],"source":["# def get_sentences(examples):\n","#     concatenatedsum(examples['input_ids'], [])\n","#     result = {\n","#         'text': []\n","#     }\n","\n","def preprocess_text(example):\n","    return tokenizer(example['text'])\n","\n","tokenized_data = data.map(preprocess_text,remove_columns=data['train'].column_names)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T22:07:37.654760Z","iopub.status.busy":"2024-02-15T22:07:37.654448Z","iopub.status.idle":"2024-02-15T22:07:38.602600Z","shell.execute_reply":"2024-02-15T22:07:38.601621Z","shell.execute_reply.started":"2024-02-15T22:07:37.654736Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 56/56 [00:00<00:00, 108.60 examples/s]\n"]}],"source":["block_size=512\n","\n","def chunk_data(examples):\n","    #print(type(examples['input_ids']))\n","    \n","    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n","    \n","    total_length = len(concatenated_examples['input_ids'])\n","    \n","    if total_length >= block_size:\n","        total_length = (total_length // block_size) * block_size\n","   \n","    result = {\n","        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n","        for k,t in concatenated_examples.items()\n","    }\n","    result['labels'] = result['input_ids'].copy()\n","   \n","    return result\n","lm_dataset = tokenized_data.map(chunk_data,batched=True,remove_columns=tokenized_data['train'].column_names)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T22:07:38.604132Z","iopub.status.busy":"2024-02-15T22:07:38.603836Z","iopub.status.idle":"2024-02-15T22:07:38.608664Z","shell.execute_reply":"2024-02-15T22:07:38.607741Z","shell.execute_reply.started":"2024-02-15T22:07:38.604106Z"},"trusted":true},"outputs":[],"source":["from transformers import DataCollatorForLanguageModeling\n","\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T22:07:38.609906Z","iopub.status.busy":"2024-02-15T22:07:38.609631Z","iopub.status.idle":"2024-02-15T22:07:39.361477Z","shell.execute_reply":"2024-02-15T22:07:39.360681Z","shell.execute_reply.started":"2024-02-15T22:07:38.609882Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["model.safetensors: 100%|██████████| 353M/353M [00:06<00:00, 54.5MB/s] \n","generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 110kB/s]\n"]}],"source":["model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T22:07:39.363660Z","iopub.status.busy":"2024-02-15T22:07:39.362845Z","iopub.status.idle":"2024-02-15T22:07:39.580220Z","shell.execute_reply":"2024-02-15T22:07:39.579393Z","shell.execute_reply.started":"2024-02-15T22:07:39.363623Z"},"trusted":true},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=\"ad-test\",\n","    evaluation_strategy=\"no\",\n","    learning_rate=1e-5,\n","    per_device_train_batch_size=16,\n","    weight_decay=0.01,\n","    push_to_hub=False,\n","    report_to=\"none\",\n","    do_eval=False, \n","    num_train_epochs=1.0,\n","    logging_strategy='epoch'\n","    \n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=lm_dataset[\"train\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer\n","    \n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["prompt = 'How I think you can value a company like tesla'\n","input_dict = tokenizer(prompt,return_tensors='pt')\n","inputs = input_dict.input_ids\n","attn = input_dict.attention_mask\n","op = model.generate(inputs.cuda(),max_new_tokens=100, do_sample=True, top_k=10, top_p=0.95,attention_mask=attn.cuda())\n","tokenizer.batch_decode(op, skip_special_tokens=True)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":891801,"sourceId":1516980,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
