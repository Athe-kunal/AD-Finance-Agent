{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q:\\AD project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO-DOS for today\n",
    "1. Build the vector database\n",
    "2. Build the basic RAG pipeline\n",
    "3. HyDE and Modified HyDE integrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD VECTOR DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 16.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# BOOK DATA\n",
    "from src.book_preprocess import get_book_data\n",
    "\n",
    "book_doc_data = get_book_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3040"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(book_doc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Let me begin this preface with a confession of a few of my own biases.\\nFirst, I believe that theory, and the models that flow from it, should provide us with the tools to understand, analyze and solve problems.\\nThe test of a model or theory then should not be based upon its elegance but upon its usefulness in problem solving.\\nSecond, there is little in corporate financial theory, in my view, that is new and revolutionary.\\nThe core principles of corporate finance are common sense ones, and have changed little over time.\\nThat should not be surprising.\\nCorporate finance is only a few decades old and people have been running businesses for thousands of years, and it would be exceedingly presumptuous of us to believe that they were in the dark until corporate finance theorists came along and told them what to do.\\nTo be fair, it is true that corporate financial theory has made advances in taking common sense principles and providing them with structure, but these advances have been primarily on the details.\\nThe story line in corporate finance has remained remarkably consistent over time.\\n\\n',\n",
       " 'page_num_coordinates': [[{'page_num': 2},\n",
       "   [248.09210205078125, 282.16666666666674],\n",
       "   [248.09210205078125, 1008.8333333333334],\n",
       "   [1459.0, 1008.8333333333334],\n",
       "   [1459.0, 282.16666666666674]]],\n",
       " 'book_source': 'Applied_corporate_finance'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_doc_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('artifacts\\YouTube_API_Transcripts\\chunked_transcripts_mba.json', 'r') as file:\n",
    "    mba_data = json.load(file)\n",
    "\n",
    "with open('artifacts\\YouTube_API_Transcripts\\chunked_transcripts_undergrad.json', 'r') as file:\n",
    "    undergrad_data = json.load(file)\n",
    "\n",
    "with open('artifacts\\YouTube_API_Transcripts\\chunked_misc_transcripts.json', 'r') as file:\n",
    "    misc_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2S_vt-N0czE', '5T2PmzsxhkM', 'lnPfpvUJEIc', '44Gv-gJxAeo', 'BA3SbC9cFIo', 't3M26iDiEfo', 'iYMsQeChsp4', 'QKQtAGmmP-4', 'eBDAnoxRKIg', 'h8AnJ3deIto', 'H5jLvqp4vas', 's4khp86YR_I', 'vXxN6A88SA4', 'CBM5iXS2pNE', 'flZcxlRtxhg', 'o7HirKysCKQ', 'F3RBKPkDQHY', 'muu0lpWns-0', 'O-sQYbULzwg', 'IFWkYXBzcpw', 'X5uFWJljqYk', 'XNQiZjoZcqY', 'gzJq6bbW-Kk', 'DQDCQxyxJ9U', 'veCrq5J4MyY', '9kid8pL8YDo'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mba_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import Document\n",
    "\n",
    "all_data_list = []\n",
    "\n",
    "curr_book = \"\"\n",
    "for book_doc in book_doc_data:\n",
    "    # print(book_doc)\n",
    "    book = book_doc['book_source']\n",
    "    if book!= curr_book:\n",
    "        curr_book = book\n",
    "        all_data_list.append(\n",
    "            Document(\n",
    "                text=book_doc['text'],\n",
    "                extra_info={\n",
    "                    'page_num_coordinates':book_doc['page_num_coordinates'],\n",
    "                    'book_source':book_doc['book_source'],\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "for json_data in [undergrad_data,mba_data,misc_data]:\n",
    "    for youtube_id, text_list in json_data.items():\n",
    "        # print(youtube_id)\n",
    "        all_data_list.append(\n",
    "            Document(\n",
    "                text=text_list[0]['text'],\n",
    "                extra_info={\n",
    "                    \"youtube_id\":youtube_id,\n",
    "                    \"start_timestamp\":text_list[0]['start_time'],\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='30f3058b-7384-433e-b1b4-25b0f84c6e1e', embedding=None, metadata={'page_num_coordinates': [[{'page_num': 2}, [248.09210205078125, 282.16666666666674], [248.09210205078125, 1008.8333333333334], [1459.0, 1008.8333333333334], [1459.0, 282.16666666666674]]], 'book_source': 'Applied_corporate_finance'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Let me begin this preface with a confession of a few of my own biases.\\nFirst, I believe that theory, and the models that flow from it, should provide us with the tools to understand, analyze and solve problems.\\nThe test of a model or theory then should not be based upon its elegance but upon its usefulness in problem solving.\\nSecond, there is little in corporate financial theory, in my view, that is new and revolutionary.\\nThe core principles of corporate finance are common sense ones, and have changed little over time.\\nThat should not be surprising.\\nCorporate finance is only a few decades old and people have been running businesses for thousands of years, and it would be exceedingly presumptuous of us to believe that they were in the dark until corporate finance theorists came along and told them what to do.\\nTo be fair, it is true that corporate financial theory has made advances in taking common sense principles and providing them with structure, but these advances have been primarily on the details.\\nThe story line in corporate finance has remained remarkably consistent over time.\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0418ffb0-4ffe-4128-982a-eac492e34dc2', embedding=None, metadata={'page_num_coordinates': [[{'page_num': 15}, [166.66666666666666, 463.84777777777765], [166.66666666666666, 1217.181111111111], [998.6877777777778, 1217.181111111111], [998.6877777777778, 463.84777777777765]], [{'page_num': 15}, [166.66666666666666, 1261.1811111111112], [166.66666666666666, 1454.6026611328125], [998.687222222222, 1454.6026611328125], [998.687222222222, 1261.1811111111112]], [{'page_num': 16}, [161.58091735839844, 170.15895080566406], [161.58091735839844, 284.34777777777765], [1003.1497192382812, 284.34777777777765], [1003.1497192382812, 170.15895080566406]]], 'book_source': 'Damodaran_on_valuation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Lord Keynes was not alone in believing that the pursuit of true value based on financial fundamentals is a fruitless one in markets where prices often seem to have little to do with value.\\nThere have always been investors in financial markets who have argued that market prices are determined by the perceptions (and misperceptions) of buyers and sellers, and not by anything as prosaic as cash flows or earnings.\\nI do not disagree with them that investor perceptions matter, but I do disagree with the notion that they are all that matter.\\nIt is a fundamental precept of this book that it is possible to estimate value from financial fundamentals, albeit with error, for most assets, and that the market price cannot deviate from this value in the long term.\\n1 From the tulip bulb craze in Holland in the early seventeenth century to the South Sea Bubble in England in the 1800s to the stock markets of the present, markets have shown the capacity to correct themselves, often at the expense of those who believed that the day of reckoning would never come.The first edition of this book was my first attempt at writing a book, and hopefully I have gained from my experiences since.\\nIn fact, this edition is very different from the prior edition for a simple reason.\\nMy other book on investment valuation, also published by John Wiley & Sons, was designed to be acomprehensive valuation book, and repeating what was said in that book here, in compressed form, strikes me as a waste of time and resources.\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='15011503-f5a8-4387-bd5e-9c69b4cc6141', embedding=None, metadata={'page_num_coordinates': [[{'page_num': 27}, [212.73696899414062, 896.5128205619643], [212.73696899414062, 1465.2627968640477], [1495.174072265625, 1465.2627968640477], [1495.174072265625, 896.5128205619643]], [{'page_num': 27}, [211.67127990722656, 1490.51953125], [211.67127990722656, 1677.7627880098808], [1493.033447265625, 1677.7627880098808], [1493.033447265625, 1490.51953125]], [{'page_num': 28}, [210.54971313476562, 294.1908264160156], [210.54971313476562, 529.846169173075], [1495.955078125, 529.846169173075], [1495.955078125, 294.1908264160156]], [{'page_num': 28}, [210.479248046875, 609.0977731105081], [210.479248046875, 900.6794870550194], [1489.2939453125, 900.6794870550194], [1489.2939453125, 609.0977731105081]], [{'page_num': 28}, [212.5172882080078, 1072.65673828125], [212.5172882080078, 1173.5961423501583], [1490.35546875, 1173.5961423501583], [1490.35546875, 1072.65673828125]], [{'page_num': 28}, [210.79100036621094, 1199.1451416015625], [210.79100036621094, 1529.846127506408], [1491.8992919921875, 1529.846127506408], [1491.8992919921875, 1199.1451416015625]], [{'page_num': 29}, [213.01968383789062, 654.7868041992188], [213.01968383789062, 696.5128288952974], [852.390625, 696.5128288952974], [852.390625, 654.7868041992188]], [{'page_num': 29}, [212.46270751953125, 785.7340087890625], [212.46270751953125, 1115.2628114473803], [1488.538818359375, 1115.2628114473803], [1488.538818359375, 785.7340087890625]], [{'page_num': 29}, [210.23403930664062, 1287.59765625], [210.23403930664062, 1475.6794630966858], [1490.0205078125, 1475.6794630966858], [1490.0205078125, 1287.59765625]], [{'page_num': 29}, [211.51995849609375, 1636.810546875], [211.51995849609375, 1825.6794485133528], [1489.8798828125, 1825.6794485133528], [1489.8798828125, 1636.810546875]], [{'page_num': 30}, [211.5013885498047, 203.18321228027344], [211.5013885498047, 438.1798396591727], [1491.890869140625, 438.1798396591727], [1491.890869140625, 203.18321228027344]], [{'page_num': 30}, [213.86717858886715, 517.4314435966058], [213.86717858886715, 713.1798282008399], [1485.4003303999834, 713.1798282008399], [1485.4003303999834, 517.4314435966058]], [{'page_num': 30}, [297.20050844997826, 740.2631604057007], [297.20050844997826, 1106.9298117945898], [1465.1325899944727, 1106.9298117945898], [1465.1325899944727, 740.2631604057007]], [{'page_num': 30}, [213.86717858886715, 1186.1814157320225], [213.86717858886715, 1381.9298003362564], [1485.0015647915989, 1381.9298003362564], [1485.0015647915989, 1186.1814157320225]], [{'page_num': 30}, [297.20050844997826, 1409.0131325411173], [297.20050844997826, 1667.3464551105612], [1463.9730483761205, 1667.3464551105612], [1463.9730483761205, 1409.0131325411173]], [{'page_num': 30}, [211.2242889404297, 1694.4297873154233], [211.2242889404297, 1975.6797755966732], [1492.2740478515625, 1975.6797755966732], [1492.2740478515625, 1694.4297873154233]], [{'page_num': 31}, [213.86717858886715, 204.77394104003906], [213.86717858886715, 294.42951231543697], [1489.818359375, 294.42951231543697], [1489.818359375, 204.77394104003906]]], 'book_source': 'Dark_Side_Valuation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Although the fundamentals of valuation are straightforward, the challenges we face in valuing companies shift as firms move through the life cycle.\\nWe go from idea businesses, often privately owned, to young growth companies, either public or on the verge of going public, to mature companies, with diverse product lines and serving different markets, to companies in decline, marking time until they are liquidated.\\nAt each stage, we are called on to estimate the same inputs—cash flows, growth rates, and discount rates—but with varying amounts of information and different degrees of precision.\\nAll too often, when confronted with significant uncertainty or limited information, we are tempted by the dark side of valuation, in which first principles are abandoned, new paradigms are created, and common sense is the casualty.This chapter begins by describing the determinants of value for any company.\\nThen it considers the estimation issues we face at each stage in the life cycle and for different types of companies.\\nWe close the chapter by looking at manifestations of the dark side of valuation.We will explore the details of valuation approaches in the next four chapters.\\nBut we can establish the determinants of value for any business without delving into the models themselves.\\nIn this section, we first consider a very simple version of an intrinsic value model, and then use this version to list the classes of inputs that determine value in any model.Intrinsic Valuation Every asset has an intrinsic value.\\nIn spite of our best efforts to observe that value, all we can do, in most cases, is arrive at an estimate of value.\\nIn discounted cash flow (DCF) valuation, the intrinsic value of an asset can be written as the present value of expected cash flows over its life, discounted to reflect both the time value of money and the riskiness of the cash flows.In this equation, E(CFt) is the expected cash flow in period t, r is the risk- adjusted discount rate for the cash flow, and N is the life of the asset.Now consider the challenges of valuing an ongoing business or company, which, in addition to owning multiple assets, also has the potential to invest in new assets in the future.\\nConsequently, not only do we have to value a portfolio of existing assets, but we also have to consider the value that might be added by new investments in the future.\\nWe can encapsulate the challenges by framing a financial balance sheet for an ongoing firm, as shown in Figure 1.1.Figure 1.1 A Financial Balance SheetThus, to value the company, we have to value both the investments already made (assets in place) and growth assets (investments that are expected in the future) while factoring in the mix of debt and equity used to fund the investments.\\nA final complication must be considered.\\nAt least in theory, a business, especially if it is publicly traded, can keep generating cash flows forever, thus requiring us to expand our consideration of cash flows to cover this perpetual life:Because estimating cash flows forever is not feasible, we simplify the process by estimating cash flows for a finite period (N) and then a “terminal value” that captures the value of all cash flows beyond that period.\\nIn effect, the equation for firm value becomes the following:Although different approaches can be used to estimate terminal value, the one most consistent with intrinsic value for a going concern is to assume that cash flows beyond year N grow at a constant rate forever, yielding the following variation on valuation:Because no firm can grow at a rate faster than the overall economy forever, this approach to estimating terminal value can be used only when the firm becomes a mature business.\\nWe examine the details of estimating the inputs —cash flows, discount rates, and growth rates—in Chapter 2, “Intrinsic Valuation.”Determinants of Value Without delving into the details of estimation, we can use the equation for the intrinsic value of the business to list the four broad questions that we need to answer in order to value any business:What are the cash flows that will be generated by the existing investments of the company? How much value, if any, will be added by future growth? How risky are the expected cash flows from both existing and growth investments, and what is the cost of funding them? When will the firm become a stable growth firm, allowing us to estimate a terminal value?What Are the Cash Flows Generated by Existing Assets? If a firm has already made significant investments, the first inputs into valuation are the cash flows from these existing assets.\\nIn practical terms, this requires estimating the following:How much the firm generated in earnings and cash flows from these assets in the most recent period How much growth (if any) is expected in these earnings/cash flows over time How long the assets will continue to generate cash flowsAlthough data that allows us to answer all these questions might be available in current financial statements, it might be inconclusive.\\nIn particular, cash flows can be difficult to obtain if the existing assets are still not fully operational (infrastructure investments that have been made but are not in full production mode) or if they are not being efficiently utilized.\\nThere can also be estimation issues when the firm in question is in a volatilebusiness, where earnings on existing assets can rise and fall as a result of macroeconomic forces.\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='927b5113-81ab-49a4-948f-d69e2eb04681', embedding=None, metadata={'page_num_coordinates': [[{'page_num': 5}, [283.7474365234375, 361.4973333333331], [283.7474365234375, 769.6868286132812], [1420.322509765625, 769.6868286132812], [1420.322509765625, 361.4973333333331]], [{'page_num': 5}, [285.3333333333333, 876.8306666666654], [285.3333333333333, 1096.163999999999], [1419.1085205078125, 1096.163999999999], [1419.1085205078125, 876.8306666666654]], [{'page_num': 5}, [285.3333333333333, 1488.8306666666658], [285.3333333333333, 1660.830666666666], [1448.0368888888881, 1660.830666666666], [1448.0368888888881, 1488.8306666666658]], [{'page_num': 5}, [285.3333333333333, 1768.1639999999993], [285.3333333333333, 1940.8306666666658], [1433.9133333333327, 1940.8306666666658], [1433.9133333333327, 1768.1639999999993]], [{'page_num': 6}, [285.3333333333333, 252.16400000000002], [285.3333333333333, 424.206298828125], [1423.4051513671875, 424.206298828125], [1423.4051513671875, 252.16400000000002]], [{'page_num': 6}, [284.5650634765625, 526.1639999999996], [284.5650634765625, 792.830666666666], [1420.92529296875, 792.830666666666], [1420.92529296875, 526.1639999999996]]], 'book_source': 'Investment_fables'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Investing is full of stories that sound good when they are told but don’t hold up under close scrutiny.\\nConsider a few: Buy stock in good companies and the returns will surely follow.\\nBuy after bad news.\\nBuy after good news.\\nStocks always win in the long term.\\nFollow the insiders.\\nBuy stocks with big dividends.\\nBuy stocks that have gone down the most.\\nGo with stocks that have gone up the most.\\nWhat makes these stories alluring is that there is a kernel of truth to each one of these stories but none of them is foolproof.\\nYou will examine these and other investment sales pitches in this book, consider the potential downside with each and how you might be able to modify each one to reduce downside risk.Human beings are much more likely to be swayed by a good story than they are by graphs and numbers.\\nThe most effective sales pitches to investors tell a compelling story, backed up by anecdotal evidence.\\nBut what makes a story compelling in the first place? Investment stories are convincing not only because they are told well but also because they draw on several common factors:In each of the chapters that follow, you will see the rationale that allows the stories presented in this book to resonate with investors through the ages.\\nAs you read these sections, you will undoubtedly remember stories you have been told by your broker, investment advisor or neighbor that are variants.Investment stories come in all forms.\\nSome are designed to appeal to investors who do not like to take risk, and they generally talk about low risk ways to play the stock market.\\nOthers are oriented towards risk seekers who want to get rich quickly; these stories emphasize the potential upside and hardly ever talk about risk.\\nStill others are structured forthose who believe that you can get something for nothing if you are smarter or better prepared than others in the market.\\nFinally, there are stories for the optimists who believe that you always win in the long term.\\nIn this section, you will get a preview of the stories that will be examined in detail in the coming chapters.Some investors are born risk averse whereas others become risk averse because of circumstances — an insecure job or impending retirement can make you far more concerned about losing money.\\nStill others are scared into risk aversion by an extended bear market.\\nWhatever the reason for the risk aversion, the investment stories that sell the best to these investors emphasize low risk strategies while promising much higher returns than they are making currently on their safe investments.\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aa5af8e0-bc66-4fde-876d-29e2dbf06822', embedding=None, metadata={'page_num_coordinates': [[{'page_num': 1}, [249.59519958496094, 330.55555555555554], [249.59519958496094, 713.8888888888889], [1452.8413244444444, 713.8888888888889], [1452.8413244444444, 330.55555555555554]]], 'book_source': 'Investment_Philosophies'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='We all dream of beating the market and being super investors and spend an inordinate amount of time and resources in this endeavor.\\nConsequently, we are easy prey for the magic bullets and the secret formulae offered by eager salespeople pushing their wares.\\nIn spite of our best efforts, most of us fail in our attempts to be more than “average” investors.\\nNonetheless, we keep trying, hoping that we can be more like the investing legends – another Warren Buffett or Peter Lynch.\\nWe read the words written by and about successful investors, hoping to find in them the key to their stock-picking abilities, so that we can replicate them and become wealthy quickly.\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3f484ead-c971-480d-8c11-bdc15c1d9115', embedding=None, metadata={'page_num_coordinates': [[{'page_num': 1}, [245.6221923828125, 340.23565673828125], [245.6221923828125, 889.635009765625], [1459.516845703125, 889.635009765625], [1459.516845703125, 340.23565673828125]], [{'page_num': 1}, [249.9425506591797, 922.69970703125], [249.9425506591797, 1065.8106689453125], [1459.220458984375, 1065.8106689453125], [1459.220458984375, 922.69970703125]]], 'book_source': 'Investment_Valuations_techniques'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Every asset, financial as well as real, has a value.\\nThe key to successfully investing in and managing these assets lies in understanding not only what the value is but also the sources of the value.\\nAny asset can be valued, but some assets are easier to value than others and the details of valuation will vary from case to case.\\nThus, the valuation of a share of a real estate property will require different information and follow a different format than the valuation of a publicly traded stock.\\nWhat is surprising, however, is not the differences in valuation techniques across assets, but the degree of similarity in basic principles.\\nThere is undeniably uncertainty associated with valuation.\\nOften that uncertainty comes from the asset being valued, though the valuation model may add to that uncertainty.This chapter lays out a philosophical basis for valuation, together with a discussion of how valuation is or can be used in a variety of frameworks, from portfolio management to corporate finance.\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2af79112-a9d7-4465-8554-aaacba3209b0', embedding=None, metadata={'page_num_coordinates': [[{'page_num': 26}, [235.78286743164062, 817.2354166666667], [235.78286743164062, 1387.6604166666664], [971.8929166666657, 1387.6604166666664], [971.8929166666657, 817.2354166666667]], [{'page_num': 27}, [237.49166666666665, 275.73263888888874], [237.49166666666665, 698.7493055555559], [972.0297916666658, 698.7493055555559], [972.0297916666658, 275.73263888888874]]], 'book_source': 'Little_Book_on_Valuation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Two Approaches to Valuation Ultimately, there are dozens of valuation models but only two valuation approaches: intrinsic and relative.\\nIn intrinsic valuation, we begin with a simple proposition: The intrin- sic value of an asset is determined by the cash flows you expect that asset to generate over its life and how uncer- tain you feel about these cash flows.\\nAssets with high and stable cash flows should be worth more than assets with low and volatile cash flows.\\nYou should pay more for a property that has long-term renters paying a high rent than for a more speculative property with not only lower rental income, but more variable vacancy rates from period to period.While the focus in principle should be on intrinsic valuation, most assets are valued on a relative basis.\\nIn relative valuation, assets are valued by looking at how the market prices similar assets.\\nThus, when determining what to pay for a house, you would look at what similar houses in the neighborhood sold for.\\nWith a stock, that means comparing its pricing to similar stocks, usually in its “peer group.” Thus, Exxon Mobil will be viewed as a stock to buy if it is trading at 8 times earnings while other oil companies trade at 12 times earnings.\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5cbf4baf-1c5e-47d2-8aa2-928650234838', embedding=None, metadata={'page_num_coordinates': [[{'page_num': 8}, [161.81707763671875, 882.2369995117188], [161.81707763671875, 1319.986328125], [1032.8740234375, 1319.986328125], [1032.8740234375, 882.2369995117188]]], 'book_source': 'Narrative_and_Numbers'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='As early as middle school, the world divides us into storytellers and num- ber crunchers, and once divided, we stay in our preferred habitats.\\nThe numbers people seek out numbers classes in school, go on to numbers dis- ciplines in college (engineering, physical sciences, accounting), and over time lose their capacity for storytelling.\\nThe storytellers populate the social science classes in school and then burnish their skills by becoming history, literature, philosophy, and psychology majors.\\nEach group learns to both fear and be suspicious of the other, and by the time they come into my valuation class as MBA students, that suspicion has deepened into a divide that seems unbridgeable.\\nYou have two tribes, each one speaking its own language and each convinced that it has a monopoly on the truth and that the other side is the one that is wrong.\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0b013b80-ddaa-4caa-bdf0-2ad5d89f19fb', embedding=None, metadata={'page_num_coordinates': [[{'page_num': 1}, [246.3429718017578, 363.6666121527778], [246.3429718017578, 1032.3331940972223], [1458.333332986111, 1032.3331940972223], [1458.333332986111, 363.6666121527778]], [{'page_num': 1}, [250.0, 1056.9998899305556], [250.0, 1378.999832986111], [1458.3333079027775, 1378.999832986111], [1458.3333079027775, 1056.9998899305556]], [{'page_num': 1}, [248.77174377441406, 1519.6665010416666], [248.77174377441406, 1784.3331663194444], [1458.3333056527777, 1784.3331663194444], [1458.3333056527777, 1519.6665010416666]], [{'page_num': 1}, [245.58200073242188, 1936.1665173541667], [245.58200073242188, 2000.1665136041668], [1456.8332623541664, 2000.1665136041668], [1456.8332623541664, 1936.1665173541667]], [{'page_num': 2}, [247.90687561035156, 204.9998065972223], [247.90687561035156, 296.3331385416666], [1458.333310597222, 296.3331385416666], [1458.333310597222, 204.9998065972223]]], 'book_source': 'Strategic_Risk'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Risk is part of every human endeavor.\\nFrom the moment we get up in the morning, drive or take public transportation to get to school or to work until we get back into our beds (and perhaps even afterwards), we are exposed to risks of different degrees.\\nWhat makes the study of risk fascinating is that while some of this risk bearing may not be completely voluntary, we seek out some risks on our own (speeding on the highways or gambling, for instance) and enjoy them.\\nWhile some of these risks may seem trivial, others make a significant difference in the way we live our lives.\\nOn a loftier note, it can be argued that every major advance in human civilization, from the caveman’s invention of tools to gene therapy, has been made possible because someone was willing to take a risk and challenge the status quo.\\nIn this chapter, we begin our exploration of risk by noting its presence through history and then look at how best to define what we mean by risk.We close the chapter by restating the main theme of this book, which is that financial theorists and practitioners have chosen to take too narrow a view of risk, in general, and risk management, in particular.\\nBy equating risk management with risk hedging, they have underplayed the fact that the most successful firms in any industry get there not by avoiding risk but by actively seeking it out and exploiting it to their own advantage.For much of human history, risk and survival have gone hand in hand.\\nPrehistoric humans lived short and brutal lives, as the search for food and shelter exposed them to physical danger from preying animals and poor weather.1 Even as more established communities developed in Sumeria, Babylon and Greece, other risks (such as war and disease) continued to ravage humanity.\\nFor much of early history, though, physical risk1 The average life span of prehistoric man was less than 30 years.\\nEven the ancient Greeks and Romans were considered aged by the time they turned 40.and material reward went hand in hand.\\nThe risk-taking caveman ended up with food and the risk-averse one starved to death.\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8db2b95b-b222-4ca3-b41a-72ac187d891a', embedding=None, metadata={'youtube_id': 'LYGYvN5LUbA', 'start_timestamp': 1.439}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"hi welcome to valuation i know many of you come in with preconceptions about what this class is about and rather than feed into those preconceptions i'd like to talk about what this class is not about and in the process perhaps perhaps you will get a sense of what this class is about i know for many of you valuation is a set of equations and formula and models and perhaps that is what you're expecting to get as science i'm sorry to disappoint you but valuation is not a science it is a craft and the essence of a craft is you learn through trial and error there is no perfect way of doing things there's a better way of doing things put simply you don't learn valuation by talking about it or watching other people talk about it so i hope that this class will help you learn valuation as a craft by doing it you will find that as you work through this class it will get easier when you start it's going to be difficult that's the nature of a craft and as you get more comfortable you'll start to get more comfortable also deviating from a script rather than cook from a recipe you'll be able to cook on your own second i know for many of you you're expecting a class built around valuing publicly traded companies you want to value the coca-colas the airbnbs the ubers the facebooks of the world and that's okay that's pretty much what many people think about valuation but i hope to make this class much more than valuing just public companies i want to talk about valuing private companies i want to talk about buying small companies and large companies develop market companies in emerging market companies and along the way what i hope you will see in this class is the principles of valuation are universal you don't really need to relearn valuation to value a specific group of companies you just need to take the existing principles evaluation and apply them differently along the way we'll talk about how value can be different from the inside when managers look at a company i suppose from the outside as investors looking at a company not because the principles change but because managers control more of the levers they can change the way a companies run by the end of this class i hope you can value just about any kind of business small or large in whatever sector in whatever market third i want to draw a contrast between two words we use interchangeably in investing value and price let me set this on the table if i believe that markets were efficient what i'm effectively saying the value and the price are pretty much the same thing and if you believe that there's really no point taking this class i believe that the value of an asset is driven by cash flows growth and risk and we're going to talk a lot about how to\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='50db26a5-9811-4320-b0bd-d17749931ae5', embedding=None, metadata={'youtube_id': '2S_vt-N0czE', 'start_timestamp': 16.92}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"so much um you thinking okay folks I see some familiar faces here so how many of you are in my corporate finance class last year not as many as I thought Okay so for many of you this is going to be your first class with me I would think so see how this goes um just to get you set for how this class will be structured at the start of every class there will be a quiz don't worry it won't be graded it won't affect your overall grade you know what the quiz will be about it's about the sub step we will do during the course of the class so it sounds like it's backwards right the squids should come after the the point of the quiz is to actually raise questions about what we're going to talk about during the course of the class and let you get a chance to actually see if you can come up with the answer in which case you're saying you know what's the point of the class and I would like that to be the end game is much of what we're going to talk in the class you could have figured out by yourself having this class might provide you a structure but we're going to start every class with a quiz so since we're going to do that every class I thought we'd start this class too with a quiz you know this class it's called valuation and there's a story behind that I'll come back and talk about that but I want to get a sense of why you're taking the class what do you expect to see in the class and I'm going to structured in in the in the form of in a bunch of questions I'm going to ask you so here's the first one and we'll come back and talk more about this but you often see people talk about valuation as an art as a science accountants claim that they've made it more scientific so my first question don't you don't have to tell me the answers I said we'll come back and talk about it if I ask you to describe valuation as a discipline would you describe it as a science and I let you define what comprises a science is it an art is it black magic white magic whatever magic you want to call it or is there something else some other word you used to describe it so file that away we'll come back and address that question sometime during the course of the classroom everybody made a choice art science magic something else second when you go into evaluation class you have a preconception about what kinds of companies you're going to wear okay and I'm going to give you you know four groups of companies and I want you to think about which of these companies is going to be easiest to value and which ones you think will give you\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3eac99dc-72ed-4939-8d9e-a0b886a75e79', embedding=None, metadata={'youtube_id': 'CdhTVs36z4c', 'start_timestamp': 0.78}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"hi welcome back in my last session I talked about banking as a business drawing a Divergence or a differential between good Banks and bad Banks and at least in this simple framework that I created a bank that has access to Sticky deposits most of which are non-interest bearing that then takes that capital and invested in loans and investment Securities where it earns an interest rate higher than what it needs to make given the default risk is a good bet and to add to the goodness if it is built the buffer up against failure with more equity and more tier one Capital all the better so dividing Banks into good Banks and bad Banks is not that difficult in this session I want to focus on a different question what banks are good Investments good banks are not necessarily good Investments and bad banks are not necessarily bad Investments if you're mystified by that it all depends on the price if a good bank is priced as a great Bank paying that high price will effectively set you up for disappointment if a bad bank is priced as an abysmal Bank getting it at The Bargain Basement price might actually make it a good investment so I want to focus on valuing and pricing banks in the session I'm going to start with valuation I like valuing companies and I like challenges in value in companies I like value young companies difficult to Value companies and banks are a different breed when it comes to valuation let's see what when you think about valuing a traditional company of two choices you can value the entire business or you can value just the equity in the business start with Equity valuation it's a little more intuitive you value the equity in a business you look at the cash flows you as Equity investors get after everybody else is being paid after interest payments after debt payments cash flow equity and you discount them back at a rate of return your demand as an equity investor given the risk and the equity or cost of equity cash flows to equity discounted the cost of equity gives you the value of equity what's the alternative you can think of lenders as capital providers just like Equity investors are you can look at the collective cash flows that Equity investors and lenders get it's a pre-dead cash flow it's still after taxes and after reinvestment but before debt payments and you discard those cash flows back at a weighted average of What lenders want which is the cost of debt and and what Equity investors want which is the cost of equity which in corporate finance is the cost of capital cash flows the business pre-dead cash flows discounted back at the cost of capital gives you the value of the business Enterprise Value value of operating assets now with typical companies you have the choice with banks you lose that choice and here's why with a non-financial Service Company\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\astar\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\astar\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\astar\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\astar\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.3 which is incompatible.\n",
      "tensorflow 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.3 which is incompatible.\n",
      "trulens-eval 0.18.1 requires pydantic<2,>=1.10.7, but you have pydantic 2.6.1 which is incompatible.\n",
      "trulens-eval 0.18.1 requires typing-extensions==4.5.0, but you have typing-extensions 4.9.0 which is incompatible.\n",
      "trulens-eval 0.18.1 requires typing-inspect==0.8.0, but you have typing-inspect 0.9.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "COLLECTION_NAME = \"ad-project\"\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=COLLECTION_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is ready\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "import openai\n",
    "# Test that your OpenAI API key is correctly set as an environment variable\n",
    "# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\n",
    "\n",
    "# Note. alternatively you can set a temporary env variable like this:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = 'sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is not None:\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    print (\"OPENAI_API_KEY is ready\")\n",
    "else:\n",
    "    print (\"OPENAI_API_KEY environment variable not found\")\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "embedding_function = OpenAIEmbeddingFunction(api_key=os.environ.get('OPENAI_API_KEY'), model_name=EMBEDDING_MODEL)\n",
    "ad_project_collection = chroma_client.create_collection(name=COLLECTION_NAME,embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "\n",
    "all_data_list = []\n",
    "\n",
    "curr_book = \"\"\n",
    "for book_doc in book_doc_data:\n",
    "    # print(book_doc)\n",
    "    book = book_doc['book_source']\n",
    "    if book!= curr_book:\n",
    "        curr_book = book\n",
    "        all_data_list.append(\n",
    "            Document(\n",
    "                page_content=book_doc['text'],\n",
    "                metadata={\n",
    "                    'page_num_coordinates':book_doc['page_num_coordinates'],\n",
    "                    'book_source':book_doc['book_source'],\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "for json_data in [undergrad_data,mba_data,misc_data]:\n",
    "    for youtube_id, text_list in json_data.items():\n",
    "        # print(youtube_id)\n",
    "        all_data_list.append(\n",
    "            Document(\n",
    "                page_content=text_list[0]['text'],\n",
    "                metadata={\n",
    "                    \"youtube_id\":youtube_id,\n",
    "                    \"start_timestamp\":text_list[0]['start_time'],\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected metadata value to be a str, int, float or bool, got [[{'page_num': 2}, [248.09210205078125, 282.16666666666674], [248.09210205078125, 1008.8333333333334], [1459.0, 1008.8333333333334], [1459.0, 282.16666666666674]]] which is a <class 'list'>\n\nTry filtering complex metadata from the document using langchain_community.vectorstores.utils.filter_complex_metadata.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\astar\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:297\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 297\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings_with_metadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts_with_metadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids_with_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\astar\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\chromadb\\api\\models\\Collection.py:477\u001b[0m, in \u001b[0;36mCollection.upsert\u001b[1;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update the embeddings, metadatas or documents for provided ids, or create them if they don't exist.\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \n\u001b[0;32m    460\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    470\u001b[0m (\n\u001b[0;32m    471\u001b[0m     ids,\n\u001b[0;32m    472\u001b[0m     embeddings,\n\u001b[0;32m    473\u001b[0m     metadatas,\n\u001b[0;32m    474\u001b[0m     documents,\n\u001b[0;32m    475\u001b[0m     images,\n\u001b[0;32m    476\u001b[0m     uris,\n\u001b[1;32m--> 477\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_embedding_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\astar\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\chromadb\\api\\models\\Collection.py:554\u001b[0m, in \u001b[0;36mCollection._validate_embedding_set\u001b[1;34m(self, ids, embeddings, metadatas, documents, images, uris, require_embeddings_or_data)\u001b[0m\n\u001b[0;32m    546\u001b[0m valid_embeddings \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    547\u001b[0m     validate_embeddings(\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_embeddings(maybe_cast_one_to_many_embedding(embeddings))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    552\u001b[0m )\n\u001b[0;32m    553\u001b[0m valid_metadatas \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 554\u001b[0m     \u001b[43mvalidate_metadatas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_cast_one_to_many_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadatas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    557\u001b[0m )\n\u001b[0;32m    558\u001b[0m valid_documents \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    559\u001b[0m     maybe_cast_one_to_many_document(documents)\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    562\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\astar\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\chromadb\\api\\types.py:291\u001b[0m, in \u001b[0;36mvalidate_metadatas\u001b[1;34m(metadatas)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metadata \u001b[38;5;129;01min\u001b[39;00m metadatas:\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mvalidate_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metadatas\n",
      "File \u001b[1;32mc:\\Users\\astar\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\chromadb\\api\\types.py:259\u001b[0m, in \u001b[0;36mvalidate_metadata\u001b[1;34m(metadata)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)):\n\u001b[1;32m--> 259\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    260\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected metadata value to be a str, int, float or bool, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which is a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    261\u001b[0m         )\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metadata\n",
      "\u001b[1;31mValueError\u001b[0m: Expected metadata value to be a str, int, float or bool, got [[{'page_num': 2}, [248.09210205078125, 282.16666666666674], [248.09210205078125, 1008.8333333333334], [1459.0, 1008.8333333333334], [1459.0, 282.16666666666674]]] which is a <class 'list'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m COLLECTION_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mad-project\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m embedding_function \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(model\u001b[38;5;241m=\u001b[39mEMBEDDING_MODEL)\n\u001b[1;32m----> 6\u001b[0m db2 \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_data_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCOLLECTION_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m docs \u001b[38;5;241m=\u001b[39m db2\u001b[38;5;241m.\u001b[39msimilarity_search(query)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# load from disk\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\astar\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:778\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    776\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    777\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[0;32m    779\u001b[0m     texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    780\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[0;32m    781\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    782\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    783\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m    784\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[0;32m    785\u001b[0m     client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[0;32m    786\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[0;32m    787\u001b[0m     collection_metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    789\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\astar\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:736\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[0;32m    731\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,\n\u001b[0;32m    732\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    733\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    734\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    735\u001b[0m     ):\n\u001b[1;32m--> 736\u001b[0m         \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    742\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[1;32mc:\\Users\\astar\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:309\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected metadata value to be\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[0;32m    305\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry filtering complex metadata from the document using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain_community.vectorstores.utils.filter_complex_metadata.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m msg)\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mValueError\u001b[0m: Expected metadata value to be a str, int, float or bool, got [[{'page_num': 2}, [248.09210205078125, 282.16666666666674], [248.09210205078125, 1008.8333333333334], [1459.0, 1008.8333333333334], [1459.0, 282.16666666666674]]] which is a <class 'list'>\n\nTry filtering complex metadata from the document using langchain_community.vectorstores.utils.filter_complex_metadata."
     ]
    }
   ],
   "source": [
    "# from langchain_community.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# query = \"What are the first steps in Valuation?\"\n",
    "# COLLECTION_NAME = \"ad-project\"\n",
    "# embedding_function = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "# db2 = Chroma.from_documents(all_data_list, embedding_function, persist_directory=COLLECTION_NAME)\n",
    "# docs = db2.similarity_search(query)\n",
    "\n",
    "# # load from disk\n",
    "# db3 = Chroma(persist_directory=COLLECTION_NAME, embedding_function=embedding_function)\n",
    "# docs = db3.similarity_search(query)\n",
    "# print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 uninstall llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.10.9-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting llama-index-agent-openai<0.2.0,>=0.1.0 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.1.4-py3-none-any.whl.metadata (695 bytes)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.8.post1 (from llama-index)\n",
      "  Downloading llama_index_core-0.10.9-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.0 (from llama-index)\n",
      "  Downloading llama_index_embeddings_openai-0.1.5-py3-none-any.whl.metadata (654 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.0 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.1.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.0 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.1.5-py3-none-any.whl.metadata (636 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.0 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.1.3-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.0 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.1.3-py3-none-any.whl.metadata (766 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.0 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.1.2-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.0 (from llama-index)\n",
      "  Downloading llama_index_readers_file-0.1.4-py3-none-any.whl.metadata (977 bytes)\n",
      "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.0 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.1.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting PyYAML>=6.0.1 (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading SQLAlchemy-2.0.27-cp39-cp39-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading aiohttp-3.9.3-cp39-cp39-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting llamaindex-py-client<0.2.0,>=0.1.13 (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading llamaindex_py_client-0.1.13-py3-none-any.whl.metadata (762 bytes)\n",
      "Collecting nest-asyncio<2.0.0,>=1.5.8 (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting numpy (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading numpy-1.26.4-cp39-cp39-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 61.0/61.0 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting openai>=1.1.0 (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading openai-1.12.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting pandas (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading pandas-2.2.0-cp39-cp39-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pillow>=9.0.0 (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading pillow-10.2.0-cp39-cp39-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting requests>=2.31.0 (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity<9.0.0,>=8.2.0 (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading tiktoken-0.6.0-cp39-cp39-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting tqdm<5.0.0,>=4.66.1 (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 57.6/57.6 kB ? eta 0:00:00\n",
      "Collecting typing-extensions>=4.5.0 (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bs4<0.0.3,>=0.0.2 (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index)\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting pymupdf<2.0.0,>=1.23.21 (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index)\n",
      "  Downloading PyMuPDF-1.23.25-cp39-none-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index)\n",
      "  Downloading pypdf-4.0.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting llama-parse<0.4.0,>=0.3.3 (from llama-index-readers-llama-parse<0.2.0,>=0.1.0->llama-index)\n",
      "  Downloading llama_parse-0.3.4-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading multidict-6.0.5-cp39-cp39-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading yarl-1.9.4-cp39-cp39-win_amd64.whl.metadata (32 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.0->llama-index)\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading wrapt-1.16.0-cp39-cp39-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting pydantic>=1.10 (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading pydantic-2.6.1-py3-none-any.whl.metadata (83 kB)\n",
      "     ---------------------------------------- 83.5/83.5 kB ? eta 0:00:00\n",
      "Collecting anyio (from httpx->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting certifi (from httpx->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading httpcore-1.0.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting idna (from httpx->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting sniffio (from httpx->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting click (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading regex-2023.12.25-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 42.0/42.0 kB ? eta 0:00:00\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting PyMuPDFb==1.23.22 (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.0->llama-index)\n",
      "  Downloading PyMuPDFb-1.23.22-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl.metadata (34 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading greenlet-3.0.3-cp39-cp39-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting colorama (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading marshmallow-3.20.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     ------------------------------------- 247.7/247.7 kB 14.8 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1 (from pandas->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio->httpx->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting packaging>=17.0 (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.2 (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading pydantic_core-2.16.2-cp39-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.8.post1->llama-index)\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading llama_index-0.10.9-py3-none-any.whl (5.6 kB)\n",
      "Downloading llama_index_agent_openai-0.1.4-py3-none-any.whl (12 kB)\n",
      "Downloading llama_index_core-0.10.9-py3-none-any.whl (15.4 MB)\n",
      "   ---------------------------------------- 15.4/15.4 MB 23.4 MB/s eta 0:00:00\n",
      "Downloading llama_index_embeddings_openai-0.1.5-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.1.2-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 2.0/2.0 MB 31.8 MB/s eta 0:00:00\n",
      "Downloading llama_index_llms_openai-0.1.5-py3-none-any.whl (9.6 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.1.3-py3-none-any.whl (6.0 kB)\n",
      "Downloading llama_index_program_openai-0.1.3-py3-none-any.whl (4.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.1.2-py3-none-any.whl (3.1 kB)\n",
      "Downloading llama_index_readers_file-0.1.4-py3-none-any.whl (36 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.1.2-py3-none-any.whl (2.7 kB)\n",
      "Downloading aiohttp-3.9.3-cp39-cp39-win_amd64.whl (366 kB)\n",
      "   --------------------------------------- 366.0/366.0 kB 22.2 MB/s eta 0:00:00\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "   ---------------------------------------- 147.9/147.9 kB ? eta 0:00:00\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "astroid 2.6.6 requires wrapt<1.13,>=1.11, but you have wrapt 1.16.0 which is incompatible.\n",
      "botocore 1.24.32 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.2.1 which is incompatible.\n",
      "degiro-connector 2.0.21 requires pandas<2.0.0,>=1.1.5, but you have pandas 2.2.0 which is incompatible.\n",
      "jupyter-server 1.23.6 requires anyio<4,>=3.1.0, but you have anyio 4.3.0 which is incompatible.\n",
      "langchain 0.0.344 requires anyio<4.0, but you have anyio 4.3.0 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.26.4 which is incompatible.\n",
      "openbb 2.5.1 requires bs4<0.0.2,>=0.0.1, but you have bs4 0.0.2 which is incompatible.\n",
      "openbb 2.5.1 requires charset-normalizer==2.1.1, but you have charset-normalizer 3.3.2 which is incompatible.\n",
      "openbb 2.5.1 requires numpy==1.23.4, but you have numpy 1.26.4 which is incompatible.\n",
      "openbb 2.5.1 requires pandas<2.0.0,>=1.5.0, but you have pandas 2.2.0 which is incompatible.\n",
      "openbb 2.5.1 requires pydantic<2.0.0,>=1.10.5, but you have pydantic 2.6.1 which is incompatible.\n",
      "openbb 2.5.1 requires tenacity<8.0.0, but you have tenacity 8.2.3 which is incompatible.\n",
      "streamlit 1.20.0 requires pandas<2,>=0.25, but you have pandas 2.2.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "   ---------------------------------------- 170.9/170.9 kB ? eta 0:00:00\n",
      "Downloading llama_parse-0.3.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading llamaindex_py_client-0.1.13-py3-none-any.whl (107 kB)\n",
      "   ---------------------------------------- 108.0/108.0 kB ? eta 0:00:00\n",
      "Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 75.9/75.9 kB ? eta 0:00:00\n",
      "Downloading httpcore-1.0.3-py3-none-any.whl (77 kB)\n",
      "   ---------------------------------------- 77.0/77.0 kB ? eta 0:00:00\n",
      "Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 1.6/1.6 MB 26.0 MB/s eta 0:00:00\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 1.5/1.5 MB 48.4 MB/s eta 0:00:00\n",
      "Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
      "   ---------------------------------------- 226.7/226.7 kB ? eta 0:00:00\n",
      "Downloading pillow-10.2.0-cp39-cp39-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 2.6/2.6 MB 23.7 MB/s eta 0:00:00\n",
      "Downloading PyMuPDF-1.23.25-cp39-none-win_amd64.whl (3.4 MB)\n",
      "   ---------------------------------------- 3.4/3.4 MB 27.1 MB/s eta 0:00:00\n",
      "Downloading PyMuPDFb-1.23.22-py3-none-win_amd64.whl (24.5 MB)\n",
      "   ---------------------------------------- 24.5/24.5 MB 18.2 MB/s eta 0:00:00\n",
      "Downloading pypdf-4.0.2-py3-none-any.whl (283 kB)\n",
      "   --------------------------------------- 284.0/284.0 kB 18.2 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 152.8/152.8 kB ? eta 0:00:00\n",
      "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 62.6/62.6 kB ? eta 0:00:00\n",
      "Downloading SQLAlchemy-2.0.27-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 2.1/2.1 MB 14.6 MB/s eta 0:00:00\n",
      "Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Downloading tiktoken-0.6.0-cp39-cp39-win_amd64.whl (798 kB)\n",
      "   --------------------------------------- 798.7/798.7 kB 12.7 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 78.3/78.3 kB ? eta 0:00:00\n",
      "Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Downloading numpy-1.26.4-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 15.8/15.8 MB 15.6 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.0-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 11.6/11.6 MB 18.2 MB/s eta 0:00:00\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "   ---------------------------------------- 85.6/85.6 kB ? eta 0:00:00\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 60.8/60.8 kB ? eta 0:00:00\n",
      "Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "   ---------------------------------------- 163.8/163.8 kB 9.6 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl (100 kB)\n",
      "   ---------------------------------------- 100.4/100.4 kB 5.6 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading frozenlist-1.4.1-cp39-cp39-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 50.7/50.7 kB ? eta 0:00:00\n",
      "Downloading greenlet-3.0.3-cp39-cp39-win_amd64.whl (290 kB)\n",
      "   --------------------------------------- 290.8/290.8 kB 18.7 MB/s eta 0:00:00\n",
      "Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "   ---------------------------------------- 61.6/61.6 kB ? eta 0:00:00\n",
      "Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 49.4/49.4 kB ? eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pydantic-2.6.1-py3-none-any.whl (394 kB)\n",
      "   --------------------------------------- 394.8/394.8 kB 24.0 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.16.2-cp39-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 1.9/1.9 MB 24.2 MB/s eta 0:00:00\n",
      "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "   --------------------------------------- 505.5/505.5 kB 15.5 MB/s eta 0:00:00\n",
      "Downloading regex-2023.12.25-cp39-cp39-win_amd64.whl (269 kB)\n",
      "   --------------------------------------- 269.5/269.5 kB 16.2 MB/s eta 0:00:00\n",
      "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "   ---------------------------------------- 345.4/345.4 kB ? eta 0:00:00\n",
      "Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "   ---------------------------------------- 121.1/121.1 kB 7.4 MB/s eta 0:00:00\n",
      "Downloading wrapt-1.16.0-cp39-cp39-win_amd64.whl (37 kB)\n",
      "Downloading yarl-1.9.4-cp39-cp39-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 76.9/76.9 kB ? eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 97.9/97.9 kB ? eta 0:00:00\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   --------------------------------------- 302.2/302.2 kB 19.5 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 58.3/58.3 kB ? eta 0:00:00\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "   ---------------------------------------- 53.0/53.0 kB ? eta 0:00:00\n",
      "Installing collected packages: pytz, dirtyjson, wrapt, urllib3, tzdata, typing-extensions, tenacity, soupsieve, sniffio, six, regex, PyYAML, PyMuPDFb, pillow, packaging, numpy, networkx, nest-asyncio, mypy-extensions, multidict, joblib, idna, h11, greenlet, fsspec, frozenlist, exceptiongroup, distro, colorama, charset-normalizer, certifi, attrs, async-timeout, annotated-types, yarl, typing-inspect, tqdm, SQLAlchemy, requests, python-dateutil, pypdf, pymupdf, pydantic-core, marshmallow, httpcore, deprecated, click, beautifulsoup4, anyio, aiosignal, tiktoken, pydantic, pandas, nltk, httpx, dataclasses-json, bs4, aiohttp, openai, llamaindex-py-client, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2023.3\n",
      "    Uninstalling pytz-2023.3:\n",
      "      Successfully uninstalled pytz-2023.3\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.12.1\n",
      "    Uninstalling wrapt-1.12.1:\n",
      "      Successfully uninstalled wrapt-1.12.1\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.9\n",
      "    Uninstalling urllib3-1.26.9:\n",
      "      Successfully uninstalled urllib3-1.26.9\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2023.3\n",
      "    Uninstalling tzdata-2023.3:\n",
      "      Successfully uninstalled tzdata-2023.3\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.2.3\n",
      "    Uninstalling tenacity-8.2.3:\n",
      "      Successfully uninstalled tenacity-8.2.3\n",
      "  Attempting uninstall: soupsieve\n",
      "    Found existing installation: soupsieve 2.3.1\n",
      "    Uninstalling soupsieve-2.3.1:\n",
      "      Successfully uninstalled soupsieve-2.3.1\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.2.0\n",
      "    Uninstalling sniffio-1.2.0:\n",
      "      Successfully uninstalled sniffio-1.2.0\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2022.3.15\n",
      "    Uninstalling regex-2022.3.15:\n",
      "      Successfully uninstalled regex-2022.3.15\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.0.1\n",
      "    Uninstalling Pillow-9.0.1:\n",
      "      Successfully uninstalled Pillow-9.0.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.0\n",
      "    Uninstalling packaging-23.0:\n",
      "      Successfully uninstalled packaging-23.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.4\n",
      "    Uninstalling numpy-1.23.4:\n",
      "      Successfully uninstalled numpy-1.23.4\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 2.7.1\n",
      "    Uninstalling networkx-2.7.1:\n",
      "      Successfully uninstalled networkx-2.7.1\n",
      "  Attempting uninstall: nest-asyncio\n",
      "    Found existing installation: nest-asyncio 1.5.5\n",
      "    Uninstalling nest-asyncio-1.5.5:\n",
      "      Successfully uninstalled nest-asyncio-1.5.5\n",
      "  Attempting uninstall: mypy-extensions\n",
      "    Found existing installation: mypy-extensions 0.4.3\n",
      "    Uninstalling mypy-extensions-0.4.3:\n",
      "      Successfully uninstalled mypy-extensions-0.4.3\n",
      "  Attempting uninstall: multidict\n",
      "    Found existing installation: multidict 5.1.0\n",
      "    Uninstalling multidict-5.1.0:\n",
      "      Successfully uninstalled multidict-5.1.0\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.3\n",
      "    Uninstalling idna-3.3:\n",
      "      Successfully uninstalled idna-3.3\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: greenlet\n",
      "    Found existing installation: greenlet 1.1.1\n",
      "    Uninstalling greenlet-1.1.1:\n",
      "      Successfully uninstalled greenlet-1.1.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.10.0\n",
      "    Uninstalling fsspec-2023.10.0:\n",
      "      Successfully uninstalled fsspec-2023.10.0\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.2.0\n",
      "    Uninstalling frozenlist-1.2.0:\n",
      "      Successfully uninstalled frozenlist-1.2.0\n",
      "  Attempting uninstall: distro\n",
      "    Found existing installation: distro 1.8.0\n",
      "    Uninstalling distro-1.8.0:\n",
      "      Successfully uninstalled distro-1.8.0\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 2.1.1\n",
      "    Uninstalling charset-normalizer-2.1.1:\n",
      "      Successfully uninstalled charset-normalizer-2.1.1\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2021.10.8\n",
      "    Uninstalling certifi-2021.10.8:\n",
      "      Successfully uninstalled certifi-2021.10.8\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.4.0\n",
      "    Uninstalling attrs-21.4.0:\n",
      "      Successfully uninstalled attrs-21.4.0\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 4.0.1\n",
      "    Uninstalling async-timeout-4.0.1:\n",
      "      Successfully uninstalled async-timeout-4.0.1\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.8.2\n",
      "    Uninstalling yarl-1.8.2:\n",
      "      Successfully uninstalled yarl-1.8.2\n",
      "  Attempting uninstall: typing-inspect\n",
      "    Found existing installation: typing-inspect 0.9.0\n",
      "    Uninstalling typing-inspect-0.9.0:\n",
      "      Successfully uninstalled typing-inspect-0.9.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.0\n",
      "    Uninstalling tqdm-4.64.0:\n",
      "      Successfully uninstalled tqdm-4.64.0\n",
      "  Attempting uninstall: SQLAlchemy\n",
      "    Found existing installation: SQLAlchemy 1.4.32\n",
      "    Uninstalling SQLAlchemy-1.4.32:\n",
      "      Successfully uninstalled SQLAlchemy-1.4.32\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.2\n",
      "    Uninstalling requests-2.28.2:\n",
      "      Successfully uninstalled requests-2.28.2\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: marshmallow\n",
      "    Found existing installation: marshmallow 3.20.1\n",
      "    Uninstalling marshmallow-3.20.1:\n",
      "      Successfully uninstalled marshmallow-3.20.1\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.2\n",
      "    Uninstalling httpcore-1.0.2:\n",
      "      Successfully uninstalled httpcore-1.0.2\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.4\n",
      "    Uninstalling click-8.0.4:\n",
      "      Successfully uninstalled click-8.0.4\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.11.1\n",
      "    Uninstalling beautifulsoup4-4.11.1:\n",
      "      Successfully uninstalled beautifulsoup4-4.11.1\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.5.0\n",
      "    Uninstalling anyio-3.5.0:\n",
      "      Successfully uninstalled anyio-3.5.0\n",
      "  Attempting uninstall: aiosignal\n",
      "    Found existing installation: aiosignal 1.2.0\n",
      "    Uninstalling aiosignal-1.2.0:\n",
      "      Successfully uninstalled aiosignal-1.2.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.7\n",
      "    Uninstalling pydantic-1.10.7:\n",
      "      Successfully uninstalled pydantic-1.10.7\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.7\n",
      "    Uninstalling nltk-3.7:\n",
      "      Successfully uninstalled nltk-3.7\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.25.2\n",
      "    Uninstalling httpx-0.25.2:\n",
      "      Successfully uninstalled httpx-0.25.2\n",
      "  Attempting uninstall: dataclasses-json\n",
      "    Found existing installation: dataclasses-json 0.6.3\n",
      "    Uninstalling dataclasses-json-0.6.3:\n",
      "      Successfully uninstalled dataclasses-json-0.6.3\n",
      "  Attempting uninstall: bs4\n",
      "    Found existing installation: bs4 0.0.1\n",
      "    Uninstalling bs4-0.0.1:\n",
      "      Successfully uninstalled bs4-0.0.1\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.9.1\n",
      "    Uninstalling aiohttp-3.9.1:\n",
      "      Successfully uninstalled aiohttp-3.9.1\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.3.7\n",
      "    Uninstalling openai-1.3.7:\n",
      "      Successfully uninstalled openai-1.3.7\n",
      "Successfully installed PyMuPDFb-1.23.22 PyYAML-6.0.1 SQLAlchemy-2.0.27 aiohttp-3.9.3 aiosignal-1.3.1 annotated-types-0.6.0 anyio-4.3.0 async-timeout-4.0.3 attrs-23.2.0 beautifulsoup4-4.12.3 bs4-0.0.2 certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.7 colorama-0.4.6 dataclasses-json-0.6.4 deprecated-1.2.14 dirtyjson-1.0.8 distro-1.9.0 exceptiongroup-1.2.0 frozenlist-1.4.1 fsspec-2024.2.0 greenlet-3.0.3 h11-0.14.0 httpcore-1.0.3 httpx-0.26.0 idna-3.6 joblib-1.3.2 llama-index-0.10.9 llama-index-agent-openai-0.1.4 llama-index-core-0.10.9 llama-index-embeddings-openai-0.1.5 llama-index-indices-managed-llama-cloud-0.1.2 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.5 llama-index-multi-modal-llms-openai-0.1.3 llama-index-program-openai-0.1.3 llama-index-question-gen-openai-0.1.2 llama-index-readers-file-0.1.4 llama-index-readers-llama-parse-0.1.2 llama-parse-0.3.4 llamaindex-py-client-0.1.13 marshmallow-3.20.2 multidict-6.0.5 mypy-extensions-1.0.0 nest-asyncio-1.6.0 networkx-3.2.1 nltk-3.8.1 numpy-1.26.4 openai-1.12.0 packaging-23.2 pandas-2.2.0 pillow-10.2.0 pydantic-2.6.1 pydantic-core-2.16.2 pymupdf-1.23.25 pypdf-4.0.2 python-dateutil-2.8.2 pytz-2024.1 regex-2023.12.25 requests-2.31.0 six-1.16.0 sniffio-1.3.0 soupsieve-2.5 tenacity-8.2.3 tiktoken-0.6.0 tqdm-4.66.2 typing-extensions-4.9.0 typing-inspect-0.9.0 tzdata-2024.1 urllib3-2.2.1 wrapt-1.16.0 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip3 install llama-index --upgrade --no-cache-dir --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'VectorStoreIndex' from 'llama_index.core' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorStoreIndex\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvector_stores\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchroma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChromaVectorStore\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StorageContext\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'VectorStoreIndex' from 'llama_index.core' (unknown location)"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb\n",
    "\n",
    "COLLECTION_NAME = \"ad-project\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "ad_project_db = chromadb.PersistentClient(path=\"ad_project_db\")\n",
    "ad_project_chroma_collection = ad_project_db.get_or_create_collection(COLLECTION_NAME)\n",
    "embed_model = OpenAIEmbedding(model=EMBEDDING_MODEL)\n",
    "vector_store = ChromaVectorStore(chroma_collection=ad_project_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    all_data_list, storage_context=storage_context, embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
